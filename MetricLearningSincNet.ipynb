{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U pytorch_metric_learning[with-hooks]==0.9.99\n",
    "#!pip install umap-learn==0.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from dnn_models import MLP, flip\n",
    "from dnn_models import SincNet as CNN \n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from pytorch_metric_learning import losses, miners, samplers, trainers, testers\n",
    "from pytorch_metric_learning.utils import common_functions\n",
    "import pytorch_metric_learning.utils.logging_presets as logging_presets\n",
    "from pytorch_metric_learning.utils.accuracy_calculator import AccuracyCalculator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "from cycler import cycler\n",
    "from tqdm import tqdm\n",
    "import record_keeper\n",
    "import pytorch_metric_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger().setLevel(logging.INFO)\n",
    "logging.info(\"VERSION %s\"%pytorch_metric_learning.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options=read_conf('SincNet_TradeDesk.cfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_lst=options.tr_lst\n",
    "te_lst=options.te_lst\n",
    "pt_file=options.pt_file\n",
    "class_dict_file=options.lab_dict\n",
    "data_folder=options.data_folder + '/'\n",
    "output_folder=options.output_folder\n",
    "\n",
    "#[windowing]\n",
    "fs=int(options.fs)\n",
    "cw_len=int(options.cw_len)\n",
    "cw_shift=int(options.cw_shift)\n",
    "\n",
    "#[cnn]\n",
    "cnn_N_filt=list(map(int, options.cnn_N_filt.split(',')))\n",
    "cnn_len_filt=list(map(int, options.cnn_len_filt.split(',')))\n",
    "cnn_max_pool_len=list(map(int, options.cnn_max_pool_len.split(',')))\n",
    "cnn_use_laynorm_inp=str_to_bool(options.cnn_use_laynorm_inp)\n",
    "cnn_use_batchnorm_inp=str_to_bool(options.cnn_use_batchnorm_inp)\n",
    "cnn_use_laynorm=list(map(str_to_bool, options.cnn_use_laynorm.split(',')))\n",
    "cnn_use_batchnorm=list(map(str_to_bool, options.cnn_use_batchnorm.split(',')))\n",
    "cnn_act=list(map(str, options.cnn_act.split(',')))\n",
    "cnn_drop=list(map(float, options.cnn_drop.split(',')))\n",
    "\n",
    "\n",
    "#[dnn]\n",
    "fc_lay=list(map(int, options.fc_lay.split(',')))\n",
    "fc_drop=list(map(float, options.fc_drop.split(',')))\n",
    "fc_use_laynorm_inp=str_to_bool(options.fc_use_laynorm_inp)\n",
    "fc_use_batchnorm_inp=str_to_bool(options.fc_use_batchnorm_inp)\n",
    "fc_use_batchnorm=list(map(str_to_bool, options.fc_use_batchnorm.split(',')))\n",
    "fc_use_laynorm=list(map(str_to_bool, options.fc_use_laynorm.split(',')))\n",
    "fc_act=list(map(str, options.fc_act.split(',')))\n",
    "\n",
    "#[class]\n",
    "class_lay=list(map(int, options.class_lay.split(',')))\n",
    "class_drop=list(map(float, options.class_drop.split(',')))\n",
    "class_use_laynorm_inp=str_to_bool(options.class_use_laynorm_inp)\n",
    "class_use_batchnorm_inp=str_to_bool(options.class_use_batchnorm_inp)\n",
    "class_use_batchnorm=list(map(str_to_bool, options.class_use_batchnorm.split(',')))\n",
    "class_use_laynorm=list(map(str_to_bool, options.class_use_laynorm.split(',')))\n",
    "class_act=list(map(str, options.class_act.split(',')))\n",
    "\n",
    "\n",
    "#[optimization]\n",
    "lr=float(options.lr)\n",
    "batch_size=int(options.batch_size)\n",
    "N_epochs=int(options.N_epochs)\n",
    "N_batches=int(options.N_batches)\n",
    "N_eval_epoch=int(options.N_eval_epoch)\n",
    "seed=int(options.seed)\n",
    "\n",
    "# sample rate\n",
    "fs = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting context and shift in samples\n",
    "wlen = int(fs * cw_len / 1000.00)\n",
    "wshift = int(fs * cw_shift / 1000.00)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize models, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extractor CNN\n",
    "trunk_arch = {\n",
    "    'input_dim': wlen,\n",
    "    'fs': fs,\n",
    "    'cnn_N_filt': cnn_N_filt,\n",
    "    'cnn_len_filt': cnn_len_filt,\n",
    "    'cnn_max_pool_len':cnn_max_pool_len,\n",
    "    'cnn_use_laynorm_inp': cnn_use_laynorm_inp,\n",
    "    'cnn_use_batchnorm_inp': cnn_use_batchnorm_inp,\n",
    "    'cnn_use_laynorm':cnn_use_laynorm,\n",
    "    'cnn_use_batchnorm':cnn_use_batchnorm,\n",
    "    'cnn_act': cnn_act,\n",
    "    'cnn_drop':cnn_drop,          \n",
    "}\n",
    "\n",
    "trunk=CNN(trunk_arch).to(device)\n",
    "trunk2=CNN(trunk_arch).to(device)\n",
    "trunk_out_dim = trunk.out_dim\n",
    "print(trunk.input_dim, '>>', trunk.out_dim)\n",
    "trunk=torch.nn.DataParallel(trunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder_arch = {\n",
    "    'input_dim': trunk_out_dim,\n",
    "    'fc_lay': fc_lay,\n",
    "    'fc_drop': fc_drop, \n",
    "    'fc_use_batchnorm': fc_use_batchnorm,\n",
    "    'fc_use_laynorm': fc_use_laynorm,\n",
    "    'fc_use_laynorm_inp': fc_use_laynorm_inp,\n",
    "    'fc_use_batchnorm_inp':fc_use_batchnorm_inp,\n",
    "    'fc_act': fc_act,\n",
    "}\n",
    "\n",
    "print(trunk_out_dim, '>>', fc_lay[-1])\n",
    "embedder=torch.nn.DataParallel(MLP(embedder_arch).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_arch = {\n",
    "    'input_dim':fc_lay[-1],\n",
    "    'fc_lay': class_lay,\n",
    "    'fc_drop': class_drop, \n",
    "    'fc_use_batchnorm': class_use_batchnorm,\n",
    "    'fc_use_laynorm': class_use_laynorm,\n",
    "    'fc_use_laynorm_inp': class_use_laynorm_inp,\n",
    "    'fc_use_batchnorm_inp':class_use_batchnorm_inp,\n",
    "    'fc_act': class_act,\n",
    "}\n",
    "\n",
    "print(fc_lay[-1], '>>', class_lay[0])\n",
    "classifier=torch.nn.DataParallel(MLP(classifier_arch).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if pt_file!='none':\n",
    "#     print('checkpoint_load')\n",
    "#     checkpoint_load = torch.load(pt_file)\n",
    "#     CNN_net.load_state_dict(checkpoint_load['trunk_model_par'])\n",
    "#     DNN1_net.load_state_dict(checkpoint_load['embedder_model_par'])\n",
    "#     DNN2_net.load_state_dict(checkpoint_load['classifier_model_par'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set optimizers\n",
    "trunk_optimizer = torch.optim.Adam(trunk.parameters(), lr=0.00001, weight_decay=0.0001)\n",
    "embedder_optimizer = torch.optim.Adam(embedder.parameters(), lr=0.0001, weight_decay=0.0001)\n",
    "classifier_optimizer = torch.optim.Adam(classifier.parameters(), lr=0.0001, weight_decay=0.0001)\n",
    "\n",
    "# trunk_optimizer = torch.optim.RMSprop(trunk.parameters(), lr=lr,alpha=0.95, eps=1e-8) \n",
    "# embedder_optimizer = torch.optim.RMSprop(embedder.parameters(), lr=lr,alpha=0.95, eps=1e-8) \n",
    "# classifier_optimizer = torch.optim.RMSprop(classifier.parameters(), lr=lr,alpha=0.95, eps=1e-8) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading label dictionary\n",
    "lab_dict = np.load(class_dict_file, allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training list\n",
    "wav_lst_tr = ReadList(tr_lst)\n",
    "snt_tr = len(wav_lst_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test list\n",
    "wav_lst_te=ReadList(te_lst)\n",
    "snt_te=len(wav_lst_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be used to create train and val sets that are class-disjoint\n",
    "class ClassAudioChunkRnd(torch.utils.data.Dataset):\n",
    "    def __init__(self, original_dataset, lab_dict, wlen=3200, data_folder = 'data', transform = None):  \n",
    "        self.data, self.targets = [], []\n",
    "        for item in original_dataset:\n",
    "            self.data.append(item)\n",
    "            self.targets.append(lab_dict.get(item, -1))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        wav, target = self.data[index], self.targets[index]\n",
    "        wav = get_rnd_chunk(os.path.join(data_folder, wav), wlen)\n",
    "        if self.transform is not None:\n",
    "            wav = self.transform(wav)\n",
    "        else:\n",
    "            wav *= np.random.uniform(0.8, 1.2)\n",
    "        wav = Variable(torch.from_numpy(wav).float().contiguous())\n",
    "        return wav, np.int64(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be used to create train and val sets that are class-disjoint\n",
    "class ClassAudioChunk(torch.utils.data.Dataset):\n",
    "    def __init__(self, original_dataset, lab_dict, wlen=3200, data_folder = 'data', transform = None):  \n",
    "        _data, _targets = [], []\n",
    "        self.data, self.targets = [], []\n",
    "        for item in original_dataset:\n",
    "            _data.append(item)\n",
    "            _targets.append(lab_dict.get(item, -1))\n",
    "        self.transform = transform\n",
    "        \n",
    "        for i in range(len(_data)):\n",
    "            path = os.path.join(data_folder, _data[i])\n",
    "            [signal, fs] = sf.read(path)\n",
    "            beg = 0\n",
    "            while beg + wlen <= len(signal) or beg == 0:\n",
    "                end = beg + wlen\n",
    "                if end > len(signal):\n",
    "                    end = len(signal)\n",
    "                self.data.append([_data[i], beg, end])\n",
    "                self.targets.append(_targets[i])\n",
    "                beg += wlen\n",
    "                    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        wav, target = self.data[index], self.targets[index]\n",
    "        [signal, fs] = sf.read(os.path.join(data_folder, wav[0]))\n",
    "        wav = signal[wav[1]:wav[2]]\n",
    "        if self.transform is not None:\n",
    "            wav = self.transform(wav)\n",
    "        else:\n",
    "            wav *= np.random.uniform(0.8, 1.2)\n",
    "        wav = Variable(torch.from_numpy(wav).float().contiguous())      \n",
    "        return wav, np.int64(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ClassAudioChunk(wav_lst_tr, lab_dict, wlen)\n",
    "val_dataset = ClassAudioChunk(wav_lst_te, lab_dict, wlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(val_dataset.targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = np.zeros([2, wlen])\n",
    "# x[0] = train_dataset[0][0]\n",
    "# x[1] = train_dataset[1][0]\n",
    "# x = Variable(torch.from_numpy(x).float().contiguous())\n",
    "# classifier(embedder(trunk(x)))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the loss, miner, sampler, and package them into dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the loss function\n",
    "loss_fn = losses.TripletMarginLoss(margin=0.1)\n",
    "# loss = losses.CrossMemory(loss_fn, 1024)\n",
    "loss = loss_fn \n",
    "\n",
    "# Set the classification loss:\n",
    "classification_loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Set the mining function\n",
    "miner = miners.MultiSimilarityMiner(epsilon=0.1)\n",
    "\n",
    "# Set the dataloader sampler\n",
    "sampler = samplers.MPerClassSampler(train_dataset.targets, m=1, length_before_new_iter=len(train_dataset))\n",
    "\n",
    "# Set other training parameters\n",
    "batch_size = 128\n",
    "\n",
    "# Package the above stuff into dictionaries.\n",
    "models = {\"trunk\": trunk, \"embedder\": embedder, \"classifier\": classifier}\n",
    "optimizers = {\"trunk_optimizer\": trunk_optimizer, \"embedder_optimizer\": embedder_optimizer, \"classifier_optimizer\": classifier_optimizer}\n",
    "loss_funcs = {\"metric_loss\": loss, \"classifier_loss\": classification_loss}\n",
    "mining_funcs = {\"tuple_miner\": miner}\n",
    "\n",
    "# We can specify loss weights if we want to. This is optional\n",
    "loss_weights = {\"metric_loss\": 1, \"classifier_loss\": 0.5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove logs if you want to train with new parameters\n",
    "# !rm -rf example_logs/ example_saved_models/ example_tensorboard/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the training and testing hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_keeper, _, _ = logging_presets.get_record_keeper(\"example_logs\", \"example_tensorboard\")\n",
    "hooks = logging_presets.get_hook_container(record_keeper)\n",
    "dataset_dict = {\"val\": val_dataset}\n",
    "model_folder = \"example_saved_models\"\n",
    "\n",
    "def visualizer_hook(umapper, umap_embeddings, labels, split_name, keyname, *args):\n",
    "    logging.info(\"UMAP plot for the {} split and label set {}\".format(split_name, keyname))\n",
    "    label_set = np.unique(labels)\n",
    "    num_classes = len(label_set)\n",
    "    fig = plt.figure(figsize=(20,15))\n",
    "    plt.gca().set_prop_cycle(cycler(\"color\", [plt.cm.nipy_spectral(i) for i in np.linspace(0, 0.9, num_classes)]))\n",
    "    for i in range(num_classes):\n",
    "        idx = labels == label_set[i]\n",
    "        plt.plot(umap_embeddings[idx, 0], umap_embeddings[idx, 1], \".\", markersize=1)   \n",
    "    plt.show()\n",
    "\n",
    "# Create the tester\n",
    "tester = testers.GlobalEmbeddingSpaceTester(end_of_testing_hook = hooks.end_of_testing_hook, \n",
    "                                            visualizer = umap.UMAP(), \n",
    "                                            visualizer_hook = visualizer_hook,\n",
    "                                            dataloader_num_workers = num_workers,\n",
    "                                            accuracy_calculator=AccuracyCalculator(k=\"max_bin_count\")\n",
    "                                           )\n",
    "\n",
    "end_of_epoch_hook = hooks.end_of_epoch_hook(tester, \n",
    "                                            dataset_dict, \n",
    "                                            model_folder, \n",
    "                                            test_interval = 1,\n",
    "                                            patience = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = trainers.TrainWithClassifier(models,\n",
    "                                optimizers,\n",
    "                                batch_size,\n",
    "                                loss_funcs,\n",
    "                                mining_funcs,\n",
    "                                train_dataset,\n",
    "                                sampler=sampler,\n",
    "                                dataloader_num_workers = num_workers,\n",
    "                                loss_weights = loss_weights,\n",
    "                                end_of_iteration_hook = hooks.end_of_iteration_hook,\n",
    "                                end_of_epoch_hook = end_of_epoch_hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting seed\n",
    "# torch.manual_seed(seed)\n",
    "# np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir example_tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trainer.train(num_epochs=num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
